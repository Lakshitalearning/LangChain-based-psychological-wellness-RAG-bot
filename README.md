# LangChain-based-psychological-wellness-RAG-bot
# ğŸ¤– AI-Powered Multimodal Chatbot with Document Q&A and Movie Recommender System

This project is a **voice + text enabled conversational assistant** that combines the power of **Large Language Models (LLMs)**, **document understanding**, and **content-based movie recommendations** into a single Streamlit web application.

It is built with:
- ğŸ§  Mistral-7B-Instruct for natural language responses
- ğŸ“„ FAISS + HuggingFace Embeddings for vectorstore and semantic search over documents respectively 
- ğŸ¬ TMDB API for movie retrieval based on keywords.
- ğŸ¤ Whisper for voice input transcription
- ğŸ’» Streamlit for a clean, user-friendly interface

---
## ğŸ“™Dataset Used 
- **The GALE ENCYCLOPEDIA of MEDICINE SECOND EDITION**
- **SELF MADE EMOTIONAL SUPPORT DATASET**
---

## ğŸ§© Project Structure 

### ğŸ“ Phase 1: Creating memory for LLM 

#### ğŸ”¹ Objective:
Enable the chatbot to answer questions based on the content of uploaded documents (PDFs).

#### ğŸ”¹ Implementation:
- **Document Loading:** PDFs are loaded using `PyPDFLoader`.
- **Text Splitting:** Documents are split into overlapping chunks using `CharacterTextSplitter` for better semantic continuity.
- **Embedding Generation:** Each chunk is embedded using HuggingFaceâ€™s `all-MiniLM-L6-v2`.
- **Vector Indexing:** FAISS is used to index and store the embeddings for efficient retrieval.

#### ğŸ”¹ Prompt Template:
Ensures that answers remain based on document context and avoids hallucination by instructing the model to say â€œI don't knowâ€ if unsure.

---

### ğŸ“ Phase 2: Connecting Memory with LLM 

#### ğŸ”¹ Objective:
Allows to connect LLM with FAISS and create QA chain

#### ğŸ”¹ Implementation:
- **Setup LLM:** Setting LLM (Mistral with HuggingFace)
- **Connect LLM+ VectorStore:** Connection of LLM and FAISS for data retrievel.
- **Loading Database:** Loading database for vectorstore directory.
- **QA Chain creation**
---

### ğŸ“ Phase 2: Voice-to-Text Interaction

#### ğŸ”¹ Objective:
Allow users to interact with the chatbot via voice in addition to text.

#### ğŸ”¹ Implementation:
- Records audio using `sounddevice`.
- Transcribes audio using OpenAIâ€™s **Whisper** model.
- Transcribed text is automatically sent to the chatbot for processing.

---

### ğŸ“ Phase 4: Streamlit UI & Styling

#### ğŸ”¹ Objective:
Deliver a clean, user-friendly interface with multi-modal input support and added user engagement.

#### ğŸ”¹ Implementation:
- **Chat Interface:** Supports both text and voice input, and retains multi-turn history via `st.session_state`.
- **File Uploader:** For uploading PDFs used in document Q&A.
- **Custom Styling:** Background and input fields styled using inline CSS.
- **Dynamic Outputs:** Chat responses, transcribed audio, and movie posters rendered in real-time.
- **Integrated Movie Recommendation System:** Seamlessly embedded the movie recommendation model into the interface. If a user mentions feeling unwell or needs a break, the chatbot suggests relevant movies based on **keywords or Title** â€” enhancing user satisfaction and engagement.

---

### ğŸ“¸ Project Screenshots

#### ğŸ”¹ Streamlit Chatbot Interface
![Streamlit Chat UI](https://github.com/user-attachments/assets/c7c3b63b-4fb4-4abb-beb5-2e362d0f12a5)




## ğŸš€ Getting Started

### ğŸ› ï¸ Prerequisites
- Python 3.9+
- Install dependencies:
```bash
pip install -r requirements.txt
